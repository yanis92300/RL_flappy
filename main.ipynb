{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import gym\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # initiate environment\n",
    "    env = gym.make(\"TextFlappyBird-v0\", height=15, width=20, pipe_gap=4)\n",
    "    obs = env.reset()\n",
    "\n",
    "    # iterate\n",
    "    while True:\n",
    "        # Select next action\n",
    "        action = (\n",
    "            env.action_space.sample()\n",
    "        )  # for an agent, action = agent.policy(observation)\n",
    "\n",
    "        # Appy action and return new observation of the environment\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Render the game\n",
    "        os.system(\"clear\")\n",
    "        sys.stdout.write(env.render())\n",
    "        time.sleep(0.2)  # FPS\n",
    "\n",
    "        # If player is dead break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "import IPython\n",
    "\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class SarsaLambdaAgent:\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict): The parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            eps_decay (float): The decay rate of the epsilon parameter,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "            trace_decay (float): The decay rate of the eligibility trace,\n",
    "            seed (int): The seed for the random number generator.\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.eps_decay = agent_init_info[\"eps_decay\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.trace_decay = agent_init_info[\"trace_decay\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_init_info[\"seed\"])\n",
    "        \n",
    "        # Initialize action-value estimates and eligibility traces\n",
    "        self.q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.e = defaultdict(lambda: np.zeros(self.num_actions))  \n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            observation (int): the state observation from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(self.q[state])\n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, state, action, next_state, next_action):\n",
    "        \"\"\"A step taken by the agent.\"\"\"\n",
    "        \n",
    "        # Update rule for Sarsa(λ)\n",
    "        delta = reward + self.discount * self.q[next_state][next_action] - self.q[state][action]\n",
    "        self.e[state][action] += 1  # Increment eligibility trace for visited state-action pair\n",
    "\n",
    "        for s in self.q:\n",
    "            for a in range(self.num_actions):\n",
    "                self.q[s][a] += self.step_size * delta * self.e[s][a]\n",
    "                self.e[s][a] *= self.discount * self.trace_decay\n",
    "\n",
    "        # Decay for the epsilon\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "\n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking.\"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)\n",
    "    \n",
    "    \n",
    "# Baseline\n",
    "agent = SarsaLambdaAgent()\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "all_reward_sums = [] # Contains sum of rewards during episode\n",
    "all_scores = [] # Contains the scores for each run\n",
    "agent_info = {\"num_actions\": 2, \"epsilon\": 0.2, \"eps_decay\":1, \"step_size\": 0.7, \"discount\": 1.0, \"trace_decay\": 0.9, \"seed\":42}\n",
    "agent.agent_init(agent_info)\n",
    "\n",
    "# Set the value that is big enough to see the converge\n",
    "num_episodes = 10000 # The number of episodes in each run\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    # Initialize done to False\n",
    "    done = False\n",
    "    reward_sums = []\n",
    "    state, info = env.reset()\n",
    "    action = agent.agent_start(state)\n",
    "    while not done:\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_action = agent.agent_start(next_state)\n",
    "        agent.agent_step(reward, state, action, next_state, next_action)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        reward_sums.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    all_reward_sums.append(np.sum(reward_sums))\n",
    "    all_scores.append(info[\"score\"])\n",
    "    \n",
    "SarsaLambda_learning_svf = agent.q\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "x = np.arange(num_episodes)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, all_reward_sums)\n",
    "plt.title(\"Sarsa(λ) Performance\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episodes\",rotation=0, labelpad=40)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average reward\n",
    "print(\"Average reward: \", np.mean(all_reward_sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilons = [0.01,0.05,0.1, 0.15,0.2,0.25,0.3]\n",
    "\n",
    "agents = {\n",
    "    \"lambda-Sarsa\": SarsaLambdaAgent()}\n",
    "\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "all_reward_sums = {} # Contains sum of rewards during episode for both algorithms\n",
    "\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    for eps in tqdm(epsilons):\n",
    "        all_reward_sums[(algorithm,eps)] = []\n",
    "\n",
    "        agent = agents[algorithm]\n",
    "        agent_info = {\"num_actions\": 2, \"epsilon\": eps, \"eps_decay\":1, \"step_size\": 0.7, \"discount\": 1,\"trace_decay\": 0.9,\"seed\":42}\n",
    "        agent.agent_init(agent_info)\n",
    "\n",
    "        for episode in range(3000):\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            reward_sums = []\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action = agent.agent_start(state)\n",
    "                next_state, reward, done, _, info = env.step(action)\n",
    "                #For SARSA to acquire the on-policy next action\n",
    "                next_action = agent.agent_start(next_state)\n",
    "                \n",
    "                agent.agent_step(reward, state, action, next_state, next_action)\n",
    "                state = next_state\n",
    "\n",
    "                reward_sums.append(reward)\n",
    "\n",
    "                # If terminal state\n",
    "                if done:\n",
    "                    break\n",
    "            all_reward_sums[(algorithm,eps)].append(np.sum(reward_sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm,eps)]) for eps in epsilons])\n",
    "    algorithm_stds = np.array([np.std(all_reward_sums[(algorithm,eps)]) for eps in epsilons])\n",
    "    plt.plot(epsilons, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n",
    "    plt.fill_between(epsilons, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\n",
    "plt.xticks(epsilons)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for trace_decay\n",
    "trace_decays = [0.3,0.5,0.7,0.9,0.99]\n",
    "\n",
    "agents = {\n",
    "    \"lambda-Sarsa\": SarsaLambdaAgent()}\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "all_reward_sums = {} # Contains sum of rewards during episode for both algorithms\n",
    "\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    for trace_decay in tqdm(trace_decays):\n",
    "        all_reward_sums[(algorithm,trace_decay)] = []\n",
    "\n",
    "        agent = agents[algorithm]\n",
    "        agent_info = {\"num_actions\": 2, \"epsilon\": 0.2, \"eps_decay\":1, \"step_size\": 0.7, \"discount\": 1,\"trace_decay\": trace_decay,\"seed\":42}\n",
    "        agent.agent_init(agent_info)\n",
    "\n",
    "        for episode in range(3000):\n",
    "            \n",
    "            # Initialize done to False\n",
    "            done = False\n",
    "            \n",
    "            reward_sums = []\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action = agent.agent_start(state)\n",
    "                next_state, reward, done, _, info = env.step(action)\n",
    "                #For SARSA to acquire the on-policy next action\n",
    "                next_action = agent.agent_start(next_state)\n",
    "                \n",
    "                agent.agent_step(reward, state, action, next_state, next_action)\n",
    "                state = next_state\n",
    "\n",
    "                reward_sums.append(reward)\n",
    "\n",
    "                # If terminal state\n",
    "                if done:\n",
    "                    break\n",
    "            all_reward_sums[(algorithm,trace_decay)].append(np.sum(reward_sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm,trace_decay)]) for trace_decay in trace_decays])\n",
    "    algorithm_stds = np.array([np.std(all_reward_sums[(algorithm,trace_decay)]) for trace_decay in trace_decays])\n",
    "    plt.plot(trace_decays, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n",
    "    plt.fill_between(trace_decays, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(\"Trace Decay\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\n",
    "plt.xticks(trace_decays)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "agents = {\n",
    "    \"lambda-Sarsa\": SarsaLambdaAgent()}\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "all_reward_sums = {} \n",
    "\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    for step_size in tqdm(step_sizes):\n",
    "        all_reward_sums[(algorithm,step_size)] = []\n",
    "\n",
    "        agent = agents[algorithm]\n",
    "        agent_info = {\"num_actions\": 2, \"epsilon\": 0.2, \"eps_decay\":1, \"step_size\": step_size, \"discount\": 1,\"trace_decay\": 0.9,\"seed\":42}\n",
    "        agent.agent_init(agent_info)\n",
    "\n",
    "        for episode in range(3000):\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            reward_sums = []\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action = agent.agent_start(state)\n",
    "                next_state, reward, done, _, info = env.step(action)\n",
    "                #For SARSA to acquire the on-policy next action\n",
    "                next_action = agent.agent_start(next_state)\n",
    "                \n",
    "                agent.agent_step(reward, state, action, next_state, next_action)\n",
    "                state = next_state\n",
    "\n",
    "                reward_sums.append(reward)\n",
    "\n",
    "                # If terminal state\n",
    "                if done:\n",
    "                    break\n",
    "            all_reward_sums[(algorithm,step_size)].append(np.sum(reward_sums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm,step_size)]) for step_size in step_sizes])\n",
    "    algorithm_stds = np.array([np.std(all_reward_sums[(algorithm,step_size)]) for step_size in step_sizes])\n",
    "    plt.plot(step_sizes, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n",
    "    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(\"Step Size\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\n",
    "plt.xticks(step_sizes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for discount\n",
    "discounts = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "agents = {\n",
    "    \"lambda-Sarsa\": SarsaLambdaAgent()}\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "all_reward_sums = {} # Contains sum of rewards during episode for both algorithms\n",
    "\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    for discount in tqdm(discounts):\n",
    "        all_reward_sums[(algorithm,discount)] = []\n",
    "\n",
    "        agent = agents[algorithm]\n",
    "        agent_info = {\"num_actions\": 2, \"epsilon\": 0.2, \"eps_decay\":1, \"step_size\": 0.7, \"discount\": discount,\"trace_decay\": 0.9,\"seed\":42}\n",
    "        agent.agent_init(agent_info)\n",
    "\n",
    "        for episode in range(3000):\n",
    "            \n",
    "            # Initialize done to False\n",
    "            done = False\n",
    "            \n",
    "            reward_sums = []\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action = agent.agent_start(state)\n",
    "                next_state, reward, done, _, info = env.step(action)\n",
    "                #For SARSA to acquire the on-policy next action\n",
    "                next_action = agent.agent_start(next_state)\n",
    "                \n",
    "                agent.agent_step(reward, state, action, next_state, next_action)\n",
    "                state = next_state\n",
    "\n",
    "                reward_sums.append(reward)\n",
    "\n",
    "                # If terminal state\n",
    "                if done:\n",
    "                    break\n",
    "            all_reward_sums[(algorithm,discount)].append(np.sum(reward_sums))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm,discount)]) for discount in discounts])\n",
    "    algorithm_stds = np.array([np.std(all_reward_sums[(algorithm,discount)]) for discount in discounts])\n",
    "    plt.plot(discounts, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n",
    "    plt.fill_between(discounts, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter combination\n",
    "\n",
    "agents = {\n",
    "    \"lambda-Sarsa\": SarsaLambdaAgent()}\n",
    "    \n",
    "\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "all_reward_sums = {} # Contains sum of rewards during episode for both algorithms\n",
    "all_scores = {} # Contains the scores for during episode for both algorithms\n",
    "all_q_tables = {}\n",
    "\n",
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    all_reward_sums[algorithm] = []\n",
    "    all_scores[algorithm] = []\n",
    "\n",
    "    agent = agents[algorithm]\n",
    "   \n",
    "    agent_info = {\"num_actions\": 2, \"epsilon\": 0.01, \"eps_decay\":1, \"step_size\": 0.3, \"discount\": 0.7, \"trace_decay\":0.3,\"seed\":42}\n",
    "\n",
    "    agent.agent_init(agent_info)\n",
    "\n",
    "    for run in tqdm(range(10000)):\n",
    "        \n",
    "        # Initialize done to False\n",
    "        done = False\n",
    "        \n",
    "        reward_sums = []\n",
    "\n",
    "        state, info = env.reset()\n",
    "        while not done:\n",
    "            action = agent.agent_start(state)\n",
    "            next_state, reward, done, _, info = env.step(action)\n",
    "            #For SARSA to acquire the on-policy next action\n",
    "            next_action = agent.agent_start(next_state)\n",
    "            \n",
    "            agent.agent_step(reward, state, action, next_state, next_action)\n",
    "            state = next_state\n",
    "\n",
    "            reward_sums.append(reward)\n",
    "\n",
    "            # If terminal state\n",
    "            if done:\n",
    "                break\n",
    "        all_reward_sums[algorithm].append(np.sum(reward_sums))\n",
    "        all_q_tables[algorithm] = agent.q\n",
    "        all_scores[algorithm].append(info[\"score\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [\"lambda-Sarsa\"]:\n",
    "    plt.plot(all_reward_sums[algorithm], label=algorithm)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot policy in 3D\n",
    "\n",
    "def plot_policy(agent_q):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = np.arange(0, 20, 1)\n",
    "    y = np.arange(0, 15, 1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros((15,20))\n",
    "    for i in range(15):\n",
    "        for j in range(20):\n",
    "            Z[i][j] = np.argmax(agent_q[(i,j)])\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    plt.show()\n",
    "    \n",
    "plot_policy(all_q_tables[\"lambda-Sarsa\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS POLICY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def plot_policy(q_values):\n",
    "    x_range = np.arange(0, 20)\n",
    "    y_range = np.arange(0, 15)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = np.zeros((15, 20))\n",
    "    for i in range(15):\n",
    "        for j in range(20):\n",
    "            Z[i][j] = np.argmax(q_values[(j, i)])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.pcolormesh(X, Y, Z, edgecolors='w', linewidth=2)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Policy\")\n",
    "    plt.show()\n",
    "plot_policy(SarsaLambda_learning_svf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" \n",
    "    env : environment\n",
    "    Q : action value function\n",
    "    epsilon : proba of random policy\n",
    "    nA : nber of actions\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs_eps_greedy(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        # take a step \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode, info[\"score\"]\n",
    "\n",
    "\n",
    "def argmax(q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        \"\"\"\n",
    "        ties = np.where(q_values == np.max(q_values))[0]\n",
    "        return np.random.choice(ties)\n",
    "\n",
    "def get_probs_eps_greedy(Q_s, epsilon, nA):\n",
    "    \"\"\" Obtains the action probabilities corresponding to an epsilon-greedy policy \n",
    "    \n",
    "    Q_s : array containing action-value functions for this state\n",
    "    epsilon : proba of random policy\n",
    "    nA : nber of actions\n",
    "\n",
    "    Returns : \n",
    "    policy_s : dictionary where policy[s] returns the action that the agent chooses after observing state s\n",
    "    \n",
    "    \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon/(nA - 1) #\n",
    "    best_a = argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon\n",
    "    return policy_s \n",
    "\n",
    "def update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode) # loop for each step of episode\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]]\n",
    "        G_i = sum(discounts[:-(i+1)] * rewards[i:])\n",
    "        Q[state][actions[i]] = old_Q + alpha*(G_i - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Graded]\n",
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1, eps_decay=.99999, eps_min=0.05):\n",
    "    start = time.time()\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "\n",
    "    return_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode, score = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        score_list.append(score)\n",
    "        _, _, rewards = zip(*episode)\n",
    "        return_list.append(sum(rewards))\n",
    "        Q = update_Q(episode, Q, alpha, gamma)\n",
    "    end = time.time()\n",
    "    print(f\"Duration of training : {end-start}\")\n",
    "    \n",
    "    policy = dict((k,argmax(v)) for k, v in Q.items())\n",
    "    \n",
    "    return policy, Q, return_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_mc, Q_mc, return_list_mc, score_list_mc = mc_control(env, 100000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean cumulative reward\n",
    "print(\"Mean cumulative reward : \", np.mean(return_list_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of rewards during episodes\n",
    "plt.plot(return_list_mc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cumulative reward\n",
    "print(\"Mean cumulative reward : \", np.mean(return_list_mc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the state value function of MC\n",
    "plot_state_value(Q_mc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy of MC in 3D\n",
    "plot_policy(Q_mc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
